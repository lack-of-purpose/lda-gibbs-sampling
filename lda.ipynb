{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim\n",
    "import nltk\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "\n",
    "# preprocess documents - lemmatization and stemming\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "def process_training_data():\n",
    "    \"\"\"Fetches training data from sklearn and preprocesses them for further modelling.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    doc_cnt\n",
    "        a number of docs in training data\n",
    "    wrd_cnt\n",
    "        a number of words in dictionary\n",
    "    docs\n",
    "        a list of preprocessed docs\n",
    "    dictionary\n",
    "        dictionary extracted from training data\n",
    "    maxdoclen\n",
    "        a length of the longest doc\n",
    "    \"\"\"\n",
    "    newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "    print(len(newsgroups_train.data), \" documents loaded.\")\n",
    "\n",
    "    print(\"Example document:\")\n",
    "    print(newsgroups_train.data[0])\n",
    "\n",
    "    processed_docs = list(map(preprocess, newsgroups_train.data))\n",
    "\n",
    "    print(\"Example document - lemmatized and stemmed:\")\n",
    "    print(processed_docs[0])\n",
    "\n",
    "\n",
    "    # Construct dictionary\n",
    "\n",
    "    dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "    dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "\n",
    "    print(\"Dictionary size: \", len(dictionary))\n",
    "\n",
    "    # Filter words in documents\n",
    "\n",
    "    docs = list()\n",
    "    maxdoclen = 0 \n",
    "    for doc in processed_docs:\n",
    "        docs.append(list(filter(lambda x: x != -1, dictionary.doc2idx(doc))))\n",
    "        maxdoclen = max(maxdoclen, len(docs[-1]))\n",
    "\n",
    "    print(\"Example document - filtered:\")\n",
    "    print(docs[0])\n",
    "\n",
    "    print(\"Maximum document length:\", maxdoclen)\n",
    "\n",
    "    doc_cnt = len(docs)\n",
    "    wrd_cnt = len(dictionary)\n",
    "    \n",
    "    return doc_cnt, wrd_cnt, docs, dictionary, maxdoclen\n",
    "\n",
    "def process_test_data(dictionary):\n",
    "    \"\"\"Fetches test data from sklearn and preprocesses them to test the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dictionary : str\n",
    "        Dictionary taken from training data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    doc_cnt_test\n",
    "        a number of docs in test data\n",
    "    docs_test\n",
    "        a list of preprocessed docs\n",
    "    \"\"\"\n",
    "    newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "    print(len(newsgroups_test.data), \" documents loaded.\")\n",
    "\n",
    "    print(\"Example document:\")\n",
    "    print(newsgroups_test.data[0])\n",
    "\n",
    "\n",
    "    # preprocess documents - lemmatization and stemming\n",
    "\n",
    "    processed_docs_test = list(map(preprocess, newsgroups_test.data))\n",
    "\n",
    "    print(\"Example document - lemmatized and stemmed:\")\n",
    "    print(processed_docs_test[0])\n",
    "\n",
    "\n",
    "    # filter words in documents\n",
    "\n",
    "    docs_test = list()\n",
    "    maxdoclen_test = 0 \n",
    "    for doc in processed_docs_test:\n",
    "        docs_test.append(list(filter(lambda x: x != -1, dictionary.doc2idx(doc))))\n",
    "        maxdoclen_test = max(maxdoclen_test, len(docs_test[-1]))\n",
    "\n",
    "    print(\"Example document - filtered:\")\n",
    "    print(docs_test[0])\n",
    "\n",
    "    print(\"Maximum document length:\", maxdoclen_test)\n",
    "\n",
    "\n",
    "    doc_cnt_test = len(docs_test)\n",
    "    \n",
    "    return doc_cnt_test, docs_test\n",
    "\n",
    "def entropy_per_topic(gamma, cw, ck, wrd_cnt, topics):\n",
    "    \"\"\"Computes entropy per topic\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gamma : float\n",
    "        Hyperparameter for Gibbs sampling\n",
    "    cw : list\n",
    "        How many times every word is assigned to topic k\n",
    "    ck : list\n",
    "        How many words are assigned to topic k\n",
    "    wrd_cnt : int\n",
    "        A number of words in dictionary\n",
    "    topics : int\n",
    "        A number of topics\n",
    "    Returns\n",
    "    -------\n",
    "    H_k\n",
    "        a list of entropies for each topic\n",
    "    \"\"\"\n",
    "    H_k = []\n",
    "    for k in range(topics):\n",
    "        probs_of_words = (cw[:,k] + gamma)/(wrd_cnt*gamma + ck[k])\n",
    "        H_k.append(-np.sum(np.multiply(probs_of_words, np.log2(probs_of_words))))\n",
    "    return H_k\n",
    "\n",
    "# set the hyperparameters for Latent Dirichlet Allocation\n",
    "iterations = 100 \n",
    "topics = 20 \n",
    "alpha = 0.01 # hyperparameter for Gibbs sampling\n",
    "gamma = 0.01 # hyperparameter for Gibbs sampling\n",
    "\n",
    "# preprocess training and test data\n",
    "doc_cnt, wrd_cnt, docs, dictionary, maxdoclen = process_training_data()\n",
    "\n",
    "doc_cnt_test, docs_test = process_test_data(dictionary)\n",
    "\n",
    "# initialize the random seed\n",
    "random.seed(13)\n",
    "\n",
    "# find the longest doc in training data\n",
    "longest_doc = 0\n",
    "length = 0\n",
    "i = 0\n",
    "for doc in docs:\n",
    "    if len(doc) > length:\n",
    "        length = len(doc)\n",
    "        longest_doc = i\n",
    "    i += 1\n",
    "\n",
    "topics_in_longest_doc = [[0 for x in range(maxdoclen)] for y in range(8)]\n",
    "\n",
    "# initialization for Gibbs sampling\n",
    "z_nd = list()\n",
    "temp = []\n",
    "for doc in docs:\n",
    "    for i in range(len(doc)):\n",
    "        temp.append(random.randint(0,19))\n",
    "    z_nd.append(temp)\n",
    "    temp = []\n",
    "\n",
    "c_d = [[0 for x in range(topics)] for y in range(doc_cnt)] # how many words in doc d are assigned to topic k\n",
    "c_w = [[0 for x in range(topics)] for y in range(wrd_cnt)] # how many times the word is assigned to topic k\n",
    "c_k = [0 for x in range(topics)] # how many words assigned to topic k\n",
    "\n",
    "i = 0\n",
    "for doc in z_nd:\n",
    "    for word in doc:\n",
    "        c_d[i][word] += 1\n",
    "    i += 1\n",
    "    \n",
    "for doc_w, doc_k in zip(docs, z_nd):\n",
    "    for word, k in zip(doc_w, doc_k):\n",
    "        c_w[word][k] += 1\n",
    "        \n",
    "for doc in z_nd:\n",
    "    for word in doc:\n",
    "        c_k[word] += 1\n",
    "\n",
    "# initial distribution over topics in the longest document\n",
    "topics_in_longest_doc[0] = copy.deepcopy(z_nd[longest_doc])\n",
    "\n",
    "save_distr = [1, 2, 5, 10, 20, 50]\n",
    "num = 1\n",
    "cw = np.asarray(c_w)\n",
    "cd = np.asarray(c_d)\n",
    "ck = np.asarray(c_k)\n",
    "\n",
    "# main loop\n",
    "K = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\n",
    "for i in range(iterations):\n",
    "    if i in save_distr:\n",
    "# save distribution over topics in the longest document after 1st, 2nd, 5th, 10th, 20th, 50th iterations\n",
    "        topics_in_longest_doc[num] = copy.deepcopy(z_nd[longest_doc])\n",
    "        num += 1\n",
    "# start compute word entropy for each topic after 1st, 2nd, 5th, 10th, 20th, 50th iteration\n",
    "        H_k = entropy_per_topic(gamma, cw, ck, wrd_cnt, topics)\n",
    "        print(f'entropy after {i}th iteration')\n",
    "        print(H_k)        \n",
    "# end word entropy computation\n",
    "    for d in range(doc_cnt):\n",
    "        for n in range(len(docs[d])):\n",
    "            z = z_nd[d][n]\n",
    "            cd[d,z] -= 1\n",
    "            w_nd = docs[d][n]\n",
    "            cw[w_nd,z] -= 1\n",
    "            ck[z] -= 1\n",
    "            pr = (alpha + cd[d,:])/(topics*alpha + len(docs[d]) - 1) * (gamma + cw[w_nd,:])/(wrd_cnt*gamma + ck)\n",
    "            sum_pr = np.sum(pr)\n",
    "            pr = pr/sum_pr\n",
    "            p = np.ndarray.tolist(pr)\n",
    "            value = random.choices(K, weights=p, k=1)\n",
    "            topic = value\n",
    "            z_nd[d][n] = topic\n",
    "            cd[d, topic] += 1\n",
    "            cw[w_nd, topic] += 1\n",
    "            ck[topic] += 1\n",
    "            \n",
    "#Perplexity of the model\n",
    "H_k = entropy_per_topic(gamma, cw, ck, wrd_cnt, topics)\n",
    "print(f'entropy after 100th iteration')\n",
    "print(H_k) \n",
    "\n",
    "entropy = np.asarray(H_k)\n",
    "Perp = 2**entropy\n",
    "print('Model perplexity')\n",
    "print(Perp)\n",
    "\n",
    "#save model\n",
    "np.savetxt('model/c_w.csv', cw, delimiter=',')\n",
    "np.savetxt('model/c_k.csv', ck, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print distribution over topics for the longest document after iterations 0,1,2,5,10,20,50,100\n",
    "topics_in_longest_doc[num] = z_nd[longest_doc]\n",
    "distribution = [[0 for x in range(topics)] for y in range(8)]\n",
    "\n",
    "i = 0\n",
    "iter = topics_in_longest_doc[0]\n",
    "for word in iter:\n",
    "        distribution[i][word] += 1\n",
    "i += 1\n",
    "for iter in range(1,len(topics_in_longest_doc)):\n",
    "    for word in topics_in_longest_doc[iter]:\n",
    "        w = word[0]\n",
    "        distribution[i][w] += 1\n",
    "    i += 1\n",
    "\n",
    "num_of_iter = [0, 1, 2, 5, 10, 20, 50, 100]\n",
    "i = 0\n",
    "for iteration in distribution:\n",
    "    print(f'{num_of_iter[i]} iteration')\n",
    "    print(iteration)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent_words(topic, words):\n",
    "    \"\"\"Prints the most frequent words for a given topic\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    topic : int\n",
    "        Topic\n",
    "    words : list\n",
    "        All words that sre assigned to a given topic\n",
    "    \"\"\"\n",
    "    dict_word_freq = {}\n",
    "    for word in words:\n",
    "        if word in dict_word_freq.keys():\n",
    "            dict_word_freq[word] += 1\n",
    "        else:\n",
    "            dict_word_freq[word] = 1\n",
    "    sorted_dict = dict(sorted(dict_word_freq.items(), key=lambda item: item[1], reverse = True))\n",
    "    d_items = sorted_dict.items()\n",
    "    i = 0\n",
    "    print(f'Words for topic {topic}')\n",
    "    for key, value in d_items:\n",
    "        print(f'Word {key} appears {value} times in topic {topic}')\n",
    "        i += 1\n",
    "        if i == 20:\n",
    "            break\n",
    "        \n",
    "# print most frequent words for 3 topics\n",
    "words_2 = []\n",
    "words_10 = []\n",
    "words_16 = []\n",
    "\n",
    "idx_to_words = {v: k for k, v in dictionary.token2id.items()}\n",
    "\n",
    "znd = []\n",
    "for doc in z_nd:\n",
    "    doc_z = []\n",
    "    for word in doc:\n",
    "        doc_z.append(word[0])\n",
    "    znd.append(doc_z)\n",
    "\n",
    "# create lists of all words that sre assigned to topics 2, 10, 16\n",
    "for doc_z, doc_w in zip(znd, docs):\n",
    "    for i in range(len(doc_z)):\n",
    "        if doc_z[i] == 2:\n",
    "            words_2.append(idx_to_words[doc_w[i]])\n",
    "        else:\n",
    "            if doc_z[i] == 10:\n",
    "                words_10.append(idx_to_words[doc_w[i]])\n",
    "            else:\n",
    "                if doc_z[i] == 16:\n",
    "                    k = doc_w[i]\n",
    "                    words_16.append(idx_to_words[doc_w[i]])\n",
    "\n",
    "most_frequent_words(2, words_2)\n",
    "most_frequent_words(10, words_10)\n",
    "most_frequent_words(16, words_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model testing \n",
    "def entropy_test(gamma, cw, cd, ck, docs, wrd_cnt, topics):\n",
    "    \"\"\"Computes entropy f test data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gamma : float\n",
    "        Hyperparameter for Gibbs sampling\n",
    "    cw : list\n",
    "        How many times every word is assigned to topic k\n",
    "    cd : list\n",
    "        How many words in doc d are assigned to topic k\n",
    "    ck : list\n",
    "        How many words are assigned to topic k\n",
    "    docs : list\n",
    "        List of all docs in test data\n",
    "    wrd_cnt : int\n",
    "        A number of words in dictionary\n",
    "    topics : int\n",
    "        A number of topics\n",
    "    Returns\n",
    "    -------\n",
    "    H\n",
    "        Entropy for test data\n",
    "    \"\"\"\n",
    "    probs_of_words = []\n",
    "    N = 0\n",
    "    for i in range(len(docs)):\n",
    "        for word in docs[i]:\n",
    "            p_k = (gamma + cw[word,:])/(wrd_cnt*gamma + ck) * (alpha + cd[i,:])/(topics*alpha + len(docs[i]))\n",
    "            probs_of_words.append(np.sum(p_k))\n",
    "        N += len(docs[i])\n",
    "    p_log = np.log2(probs_of_words)\n",
    "    \n",
    "    sum_log = 0\n",
    "    for doc in docs:\n",
    "        for word in doc:\n",
    "            sum_log += p_log[word]\n",
    "        \n",
    "    H = - 1/N * sum_log\n",
    "    return H\n",
    "\n",
    "#initialization\n",
    "z_nd_test = list()\n",
    "temp = []\n",
    "for doc in docs_test:\n",
    "    for i in range(len(doc)):\n",
    "        temp.append(random.randint(0,19))\n",
    "    z_nd_test.append(temp)\n",
    "    temp = []\n",
    "\n",
    "c_d_test = [[0 for x in range(topics)] for y in range(doc_cnt_test)] #how many words in doc d are assigned to topic k\n",
    "c_w = np.loadtxt('model/cw.csv', delimiter=',')\n",
    "c_k = np.loadtxt('model/ck.csv')\n",
    "\n",
    "i = 0\n",
    "for doc in z_nd_test:\n",
    "    for word in doc:\n",
    "        c_d_test[i][word] += 1\n",
    "    i += 1\n",
    "\n",
    "cd_test = np.asarray(c_d_test)\n",
    "\n",
    "# main loop\n",
    "K = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\n",
    "for i in range(50):\n",
    "    for d in range(doc_cnt_test):\n",
    "        for n in range(len(docs_test[d])):\n",
    "            z = z_nd_test[d][n]\n",
    "            cd_test[d,z] -= 1\n",
    "            pr = (alpha + cd_test[d,:])/(topics*alpha + len(docs_test[d]) - 1) * (gamma + cw[w_nd,:])/(wrd_cnt*gamma + ck)\n",
    "            sum_pr = np.sum(pr)\n",
    "            pr = pr/sum_pr\n",
    "            p = np.ndarray.tolist(pr)\n",
    "            value = random.choices(K, weights=p, k=1)\n",
    "            topic = value\n",
    "            z_nd_test[d][n] = topic\n",
    "            cd_test[d, topic] += 1 \n",
    "\n",
    "#Perplexity of the model\n",
    "H_test = entropy_test(gamma, c_w, cd_test, c_k, docs_test, wrd_cnt, topics)\n",
    "print(f'test entropy after 50th iteration')\n",
    "print(H_test) \n",
    "\n",
    "PP_test = 2**H_test\n",
    "print('Test model perplexity')\n",
    "print(PP_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple bayesian model using only one distribution over words for all documents with symmetric Dirichlet prior (to compare with LDA)\n",
    "\n",
    "c_w = np.loadtxt('model/cw.csv', delimiter=',')\n",
    "\n",
    "N = 0\n",
    "for doc_d in docs_test:\n",
    "    N += len(doc_d)\n",
    "    \n",
    "gamma_simple = 0.1\n",
    "cw_w = np.sum(c_w, axis=1)\n",
    "pw = (gamma_simple + cw_w)/(wrd_cnt*gamma_simple + np.sum(cw_w))\n",
    "log_pw = np.log2(pw)\n",
    "\n",
    "\n",
    "sum_log = 0\n",
    "for doc in docs_test:\n",
    "    for word in doc:\n",
    "        sum_log += log_pw[word]\n",
    "\n",
    "H_simple = - 1/N * sum_log\n",
    "\n",
    "PP_simple = 2**H_simple\n",
    "print('Simple bayesian model perplexity')\n",
    "print(PP_simple)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2305800fefceda3f2e2d019a83e737014c066fed7151de411f61bfc53e80d6fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
